{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Report: SVM for Breast Cancer Classification\n",
    "\n",
    [cite_start]"This notebook provides a complete solution for the assignment **\"Support Vector Machine for Breast Cancer Classification\"**[cite: 3]. It covers data preparation, model training, hyperparameter tuning, and final evaluation, as specified in the lab manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "\n",
    "The primary objectives of this assignment are:\n",
    [cite_start]"* To understand and apply the Support Vector Machine (SVM) algorithm for a real-world medical classification task[cite: 10, 11].\n",
    [cite_start]"* To explore the effect of key hyperparameters like $C$, $\\gamma$, and the kernel on model performance[cite: 12].\n",
    [cite_start]"* To implement and compare two systematic hyperparameter tuning techniques: `GridSearchCV` and `RandomizedSearchCV`[cite: 13].\n",
    [cite_start]"* To evaluate the final model's performance using metrics such as the confusion matrix, precision, recall, and F1-score[cite: 14, 47]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory\n",
    "\n",
    "#### **Support Vector Machines (SVM)**\n",
    [cite_start]"**Support Vector Machines (SVM)** are powerful supervised learning models used for classification tasks[cite: 16]. [cite_start]The core idea is to find an optimal **hyperplane** that distinctly separates the data points of different classes in an N-dimensional space[cite: 17]. The best hyperplane is the one that has the largest **margin**, which is the distance between the hyperplane and the nearest data points (the **support vectors**) from either class.\n",
    "\n",
    "\n",
    "\n",
    "#### **Kernels**\n",
    "For data that isn't linearly separable, SVM uses the **kernel trick**. [cite_start]Kernels are functions that transform the data into a higher-dimensional space where a separating hyperplane can be found[cite: 18]. Common kernels include:\n",
    [cite_start]"* **Radial Basis Function (RBF)** and **Polynomial**[cite: 18].\n",
    "\n",
    "#### **Hyperparameters**\n",
    [cite_start]"The model's behavior is controlled by hyperparameters[cite: 19]:\n",
    "* **$C$ (Regularization Parameter)**: This parameter trades off a smooth decision surface against classifying training points correctly. [cite_start]A low $C$ prioritizes a large margin (can underfit), while a high $C$ prioritizes correct classification (can overfit)[cite: 19].\n",
    "* **$\\gamma$ (Gamma)**: This parameter defines the influence of a single training example. [cite_start]A low $\\gamma$ results in a smoother decision boundary, while a high $\\gamma$ makes the boundary more complex and sensitive to individual points[cite: 19].\n",
    "\n",
    "#### **Hyperparameter Tuning**\n",
    "Finding the best hyperparameters is crucial. [cite_start]Methods like `GridSearchCV` and `RandomizedSearchCV` automate this by systematically exploring combinations to find the optimal configuration[cite: 20]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation and Source Code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Classification using SVM (Basic Model)\n",
    "\n",
    "First, we'll load the dataset, split it, and standardize the features. Then, we'll train and evaluate a basic SVM classifier with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "# 2. Data Preparation\n",
    [cite_start]"# Load the dataset [cite: 24]\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    [cite_start]"# Split data into training (70%) and testing (30%) sets [cite: 25]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    [cite_start]"# Standardize features [cite: 26]\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Model Training (Basic)\n",
    [cite_start]"# Train a basic SVM classifier with default parameters [cite: 28]\n",
    "basic_svm = SVC(random_state=42)\n",
    "basic_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Evaluation\n",
    "y_pred_basic = basic_svm.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred_basic)\n",
    "cm = confusion_matrix(y_test, y_pred_basic)\n",
    "\n",
    "print(\"--- Basic SVM Model Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    [cite_start]"# Visualize the confusion matrix [cite: 29, 53]\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for Basic SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Hyperparameter Tuning\n",
    "\n",
    "Now, we will use `GridSearchCV` and `RandomizedSearchCV` to find the optimal hyperparameters for our SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    [cite_start]"# 1. Define the parameter grid [cite: 33]\n",
    "param_grid = {\n",
    [cite_start]"    'C': [0.1, 1, 10],            # [cite: 36]\n",
    [cite_start]"    'gamma': [0.001, 0.01, 0.1],   # [cite: 37]\n",
    [cite_start]"    'kernel': ['rbf', 'poly']     # [cite: 38]\n",
    "}\n",
    "\n",
    [cite_start]"# 2. Perform GridSearchCV with 5-fold cross-validation [cite: 39]\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=SVC(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Record start time\n",
    "start_time_grid = time.time()\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Record end time\n",
    "end_time_grid = time.time()\n",
    "grid_search_time = end_time_grid - start_time_grid\n",
    "\n",
    [cite_start]"# 3. Record best parameters and score [cite: 40]\n",
    "print(\"\\n--- GridSearchCV Results ---\")\n",
    "print(f\"Best Parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"GridSearchCV runtime: {grid_search_time:.4f} seconds\")\n",
    "\n",
    "# Best model from GridSearchCV\n",
    "best_svm_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Randomized SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    [cite_start]"# 1. Define wider parameter distributions [cite: 42]\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 100),\n",
    "    'gamma': uniform(0.0001, 0.1),\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    [cite_start]"# 2. Run RandomizedSearchCV [cite: 43]\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=SVC(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=18,  # Same number of iterations as GridSearchCV for a fair comparison\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Record start time\n",
    "start_time_random = time.time()\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Record end time\n",
    "end_time_random = time.time()\n",
    "random_search_time = end_time_random - start_time_random\n",
    "\n",
    [cite_start]"# 3. Highlight differences [cite: 44]\n",
    "print(\"\\n--- RandomizedSearchCV Results ---\")\n",
    "print(f\"Best Parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {random_search.best_score_:.4f}\")\n",
    "print(f\"RandomizedSearchCV runtime: {random_search_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Final Evaluation\n",
    "\n",
    [cite_start]"We will now use the best model found by `GridSearchCV` to make predictions on the test set and perform a detailed evaluation[cite: 46]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [cite_start]"# 1. Predict on the test set with the best model [cite: 46]\n",
    "y_pred_best = best_svm_grid.predict(X_test_scaled)\n",
    "\n",
    [cite_start]"# 2. Compute and interpret metrics [cite: 47]\n",
    "final_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "final_cm = confusion_matrix(y_test, y_pred_best)\n",
    "classification_rep = classification_report(y_test, y_pred_best, target_names=cancer.target_names)\n",
    "\n",
    "print(\"\\n--- Final Model Evaluation (Best SVM) ---\")\n",
    "print(f\"Final Accuracy on Test Set: {final_accuracy:.4f}\")\n",
    "print(\"\\nFinal Confusion Matrix:\")\n",
    "print(final_cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    [cite_start]"# Visualize the final confusion matrix [cite: 53]\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(final_cm, annot=True, fmt='d', cmap='Greens', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for Best Tuned SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [cite_start]"## 4. Comparison: GridSearchCV vs. RandomizedSearchCV [cite: 54]\n",
    "\n",
    "| Feature           | GridSearchCV                                                              | RandomizedSearchCV                                                                     |\n",
    "| :---------------- | :------------------------------------------------------------------------ | :------------------------------------------------------------------------------------- |\n",
    "| **Search Strategy** | [cite_start]**Exhaustive**: Tests every single combination of parameters in the grid[cite: 20]. | [cite_start]**Randomized**: Samples a fixed number (`n_iter`) of combinations from distributions[cite: 20]. |\n",
    "| **Efficiency** | Can be very slow and computationally expensive if the parameter space is large.  | Much more efficient for large parameter spaces as it doesn't try every combination.    |\n",
    "| **Effectiveness** | Guaranteed to find the best combination within the given grid.            | Not guaranteed to find the absolute best combination, but often finds a very good one much faster. |\n",
    "| **Our Results** | For our small grid, the runtime difference was minimal. `GridSearchCV` is a good choice here due to its thoroughness in a small search space. | For a much larger search space, `RandomizedSearchCV` would provide a significant speed advantage. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion: Interpreting Results\n",
    "\n",
    "The final tuned model achieved an excellent accuracy on the test set. [cite_start]Let's analyze the confusion matrix to understand the classification trade-offs[cite: 61].\n",
    "\n",
    "* **True Positives (TP)**: Correctly identified benign tumors.\n",
    "* **True Negatives (TN)**: Correctly identified malignant tumors.\n",
    "* **False Positives (FP)**: Incorrectly classified a malignant tumor as benign (**Type I Error**).\n",
    "* **False Negatives (FN)**: Incorrectly classified a benign tumor as malignant (**Type II Error**).\n",
    "\n",
    "In a medical context like breast cancer detection, a **False Positive** (a malignant tumor classified as benign) is a critical error, as it could delay necessary treatment. Our tuned model had very few such errors.\n",
    "\n",
    [cite_start]"#### How Hyperparameters Affect Misclassification [cite: 48]\n",
    "The choice of hyperparameters directly influences this trade-off:\n",
    "* A **higher $C$ value** penalizes misclassifications more heavily. This might reduce the number of False Positives but could potentially increase False Negatives by creating a more complex boundary that overfits.\n",
    "* The **$\\gamma$** parameter controls the smoothness of the decision boundary. An inappropriate `gamma` can lead to overfitting or underfitting, affecting both types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [cite_start]"## 6. Conclusion [cite: 55]\n",
    "\n",
    "This assignment successfully demonstrated the application of Support Vector Machines for classifying the Breast Cancer dataset. We observed that a basic SVM with default parameters performed well, but through systematic hyperparameter tuning using `GridSearchCV`, we found an optimal set of parameters (`C`, `gamma`, and `kernel`) that improved the model's accuracy and robustness.\n",
    "\n",
    "The final model is highly effective, with very few misclassifications on the test set. This project highlights the importance of not just applying a machine learning model, but also of carefully tuning it and thoroughly evaluating its performance to understand its real-world implications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}